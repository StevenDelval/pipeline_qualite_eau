{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a24a6478",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57fbc08",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f069e013",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83f33a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "STORAGE_ACCOUNT_NAME = os.environ[\"STORAGE_ACCOUNT_NAME\"]\n",
    "FILESYSTEM_NAME = os.environ[\"CONTAINER_NAME\"]\n",
    "FILESYSTEM_NAME_BRONZE = os.environ[\"CONTAINER_BRONZE\"]\n",
    "SECRET_SCOPE_NAME = os.environ[\"SECRET_SCOPE_NAME\"]\n",
    "SECRET_KEY_NAME = os.environ[\"SECRET_KEY_NAME\"]\n",
    "MOUNT_POINT = \"/mnt/donnees-qualite-eau\"\n",
    "MOUNT_POINT_BRONZE = \"/mnt/donnees-qualite-eau-bronze\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1211f5b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Configuration du logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a5b9f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb628c",
   "metadata": {},
   "source": [
    "# Vérifier / Créer le montage du Data Lake si nécessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a14887",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def ensure_mount(mount_point: str, container_name: str):\n",
    "    \"\"\"\n",
    "    Vérifie si un point de montage Databricks existe, et le crée si nécessaire.\n",
    "\n",
    "    Args:\n",
    "        mount_point (str): Chemin du point de montage local dans Databricks (ex: '/mnt/datalake').\n",
    "        container_name (str): Nom du conteneur Blob Storage à monter.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Si la création du montage échoue.\n",
    "\n",
    "    Remarques:\n",
    "        - Utilise les variables globales SCOPE, KEY_NAME et STORAGE_ACCOUNT pour accéder aux secrets et au compte de stockage.\n",
    "        - Si le montage existe déjà, la fonction ne fait rien.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mounts = [m.mountPoint for m in dbutils.fs.mounts()]\n",
    "        if mount_point in mounts:\n",
    "            logger.info(f\"Mount already exists: {mount_point}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Mount does not exist. Creating mount at {mount_point}...\")\n",
    "        storage_key = dbutils.secrets.get(scope=SECRET_SCOPE_NAME, key=SECRET_KEY_NAME)\n",
    "\n",
    "        configs = {\n",
    "            f\"fs.azure.account.key.{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\": storage_key\n",
    "        }\n",
    "\n",
    "        dbutils.fs.mount(\n",
    "            source=f\"wasbs://{container_name}@{STORAGE_ACCOUNT_NAME}.blob.core.windows.net/\",\n",
    "            mount_point=mount_point,\n",
    "            extra_configs=configs\n",
    "        )\n",
    "        logger.info(f\"Mount created successfully at {mount_point}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to ensure mount: {e}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc602eb0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Crée les montages si besoin\n",
    "ensure_mount(MOUNT_POINT,FILESYSTEM_NAME)\n",
    "ensure_mount(MOUNT_POINT_BRONZE,FILESYSTEM_NAME_BRONZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ed3ddf",
   "metadata": {},
   "source": [
    "# Fonction utilitaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99875bb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def read_txt_files_spark(prefix: str, sep: str = \",\", header: bool = True):\n",
    "    \"\"\"\n",
    "    Lit tous les fichiers TXT correspondant à un préfixe depuis le dossier monté avec Spark.\n",
    "    Args:\n",
    "        prefix (str): Préfixe des fichiers à lire (ex: '2025_10_').\n",
    "        sep (str): Séparateur utilisé dans les fichiers TXT (par défaut ',').\n",
    "        header (bool): Indique si la première ligne contient les noms de colonnes.\n",
    "    Returns:\n",
    "        pyspark.sql.DataFrame: DataFrame Spark contenant les données chargées.\n",
    "    \"\"\"\n",
    "    path_pattern = f\"{MOUNT_POINT}/{prefix}*.txt\"\n",
    "\n",
    "    try:\n",
    "        df = (\n",
    "            spark.read.format(\"csv\")  # 'csv' marche aussi pour les fichiers .txt\n",
    "            .option(\"header\", str(header).lower())\n",
    "            .option(\"inferSchema\", \"true\")\n",
    "            .option(\"delimiter\", sep)\n",
    "            .load(path_pattern)\n",
    "        )\n",
    "\n",
    "        # Ajouter le nom du fichier source comme colonne\n",
    "        df = df.withColumn(\"source_file\", F.input_file_name())\n",
    "        \n",
    "        df = df.withColumn(\n",
    "            \"annee\",\n",
    "            F.regexp_extract(F.col(\"source_file\"), r\"dis-(\\d{4})\", 1)\n",
    "        )\n",
    "\n",
    "        n = df.count()\n",
    "        logger.info(f\"{n} lignes chargées pour le préfixe {prefix}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lecture fichiers {path_pattern}: {e}\", exc_info=True)\n",
    "        return spark.createDataFrame([], schema=None)  # DataFrame vide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b400225",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def write_df_to_table(df, table_name: str, mode: str = \"overwrite\", partition_col: str = None):\n",
    "    \"\"\"\n",
    "    Écrit un DataFrame Spark dans une table Delta Databricks.\n",
    "    Permet de partitionner la table par une colonne.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Le DataFrame Spark à stocker.\n",
    "        table_name (str): Nom complet de la table cible (ex: 'bronze.txt_table').\n",
    "        mode (str): Mode d’écriture Spark ('overwrite', 'append', 'ignore', 'error').\n",
    "        partition_col (str, optional): Nom de la colonne pour partitionner la table (ex: 'annee').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if df.isEmpty():\n",
    "            logger.warning(f\"Aucune donnée à écrire dans la table '{table_name}'.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Écriture dans la table Delta '{table_name}' (mode={mode})...\")\n",
    "\n",
    "        writer = df.write.format(\"delta\").mode(mode).option(\"overwriteSchema\", \"true\")\n",
    "        if partition_col:\n",
    "            writer = writer.partitionBy(partition_col)\n",
    "\n",
    "        writer.saveAsTable(table_name)\n",
    "\n",
    "        logger.info(f\"Données stockées avec succès dans '{table_name}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'écriture dans la table '{table_name}': {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a947e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def write_df_to_parquet(df, path: str, mode: str = \"overwrite\"):\n",
    "    \"\"\"\n",
    "    Écrit un DataFrame Spark en format Parquet dans le Data Lake.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Le DataFrame Spark à écrire.\n",
    "        path (str): Chemin complet dans le Data Lake (ex: '/mnt/datalake/bronze/dis_plv/').\n",
    "        mode (str): Mode d’écriture ('overwrite', 'append', etc.).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if df.isEmpty():\n",
    "            logger.warning(f\"Aucune donnée à écrire dans {path}.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Écriture en Parquet dans {path} (mode={mode})...\")\n",
    "        df.write.mode(mode).parquet(path)\n",
    "        logger.info(f\"Données stockées avec succès dans {path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur écriture Parquet dans {path}: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0af94",
   "metadata": {},
   "source": [
    "# Config Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2be7130",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb9431",
   "metadata": {},
   "source": [
    "# Extraction et insertion en tables bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd2320",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_plv = read_txt_files_spark(\"dis-*/DIS_PLV_\")\n",
    "df_result = read_txt_files_spark(\"dis-*/DIS_RESULT_\")\n",
    "df_com = read_txt_files_spark(\"dis-*/DIS_COM_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2441ad69",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "write_df_to_table(df_plv, \"bronze.dis_plv\", mode=\"overwrite\", partition_col=\"annee\")\n",
    "write_df_to_table(df_result, \"bronze.dis_result\", mode=\"overwrite\", partition_col=\"annee\")\n",
    "write_df_to_table(df_com, \"bronze.dis_com\", mode=\"overwrite\", partition_col=\"annee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc333525",
   "metadata": {},
   "source": [
    "# Copie en parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00a65b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "write_df_to_parquet(df_plv, f\"{MOUNT_POINT_BRONZE}/dis_plv/\")\n",
    "write_df_to_parquet(df_result, f\"{MOUNT_POINT_BRONZE}/dis_result/\")\n",
    "write_df_to_parquet(df_com, f\"{MOUNT_POINT_BRONZE}/dis_com/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
