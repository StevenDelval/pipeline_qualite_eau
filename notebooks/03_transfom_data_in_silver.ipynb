{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edc95968",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a0a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e037fa3",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d3fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "STORAGE_ACCOUNT_NAME = os.environ[\"STORAGE_ACCOUNT_NAME\"]\n",
    "FILESYSTEM_NAME_SILVER = os.environ[\"CONTAINER_SILVER\"]\n",
    "SECRET_SCOPE_NAME = os.environ[\"SECRET_SCOPE_NAME\"]\n",
    "SECRET_KEY_NAME = os.environ[\"SECRET_KEY_NAME\"]\n",
    "MOUNT_POINT_SILVER = \"/mnt/donnees-qualite-eau-silver\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ea1db",
   "metadata": {},
   "source": [
    "# Configuration du logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dedd04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79331e9a",
   "metadata": {},
   "source": [
    "# Vérifier / Créer le montage du Data Lake si nécessaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ae7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_mount(mount_point: str, container_name: str):\n",
    "    \"\"\"\n",
    "    Vérifie si un point de montage Databricks existe, et le crée si nécessaire.\n",
    "\n",
    "    Args:\n",
    "        mount_point (str): Chemin du point de montage local dans Databricks (ex: '/mnt/datalake').\n",
    "        container_name (str): Nom du conteneur Blob Storage à monter.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Si la création du montage échoue.\n",
    "\n",
    "    Remarques:\n",
    "        - Utilise les variables globales SCOPE, KEY_NAME et STORAGE_ACCOUNT pour accéder aux secrets et au compte de stockage.\n",
    "        - Si le montage existe déjà, la fonction ne fait rien.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mounts = [m.mountPoint for m in dbutils.fs.mounts()]\n",
    "        if mount_point in mounts:\n",
    "            logger.info(f\"Mount already exists: {mount_point}\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Mount does not exist. Creating mount at {mount_point}...\")\n",
    "        storage_key = dbutils.secrets.get(scope=SECRET_SCOPE_NAME, key=SECRET_KEY_NAME)\n",
    "\n",
    "        configs = {\n",
    "            f\"fs.azure.account.key.{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\": storage_key\n",
    "        }\n",
    "\n",
    "        dbutils.fs.mount(\n",
    "            source=f\"wasbs://{container_name}@{STORAGE_ACCOUNT_NAME}.blob.core.windows.net/\",\n",
    "            mount_point=mount_point,\n",
    "            extra_configs=configs\n",
    "        )\n",
    "        logger.info(f\"Mount created successfully at {mount_point}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to ensure mount: {e}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11430ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crée les montages si besoin\n",
    "ensure_mount(MOUNT_POINT_SILVER,FILESYSTEM_NAME_SILVER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69550858",
   "metadata": {},
   "source": [
    "# Fonction utilitaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459660ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_com_silver(df):\n",
    "    # Renommer les colonnes\n",
    "    df = (\n",
    "        df\n",
    "        .withColumnRenamed(\"Inseecommune\", \"insee_commune\")\n",
    "        .withColumnRenamed(\"Nomcommune\", \"nom_commune\")\n",
    "        .withColumnRenamed(\"Quartier\", \"quartier\")\n",
    "        .withColumnRenamed(\"Cdreseau\", \"cd_reseau\")\n",
    "        .withColumnRenamed(\"Nomreseau\", \"nom_reseau\")\n",
    "        .withColumnRenamed(\"Debutalim\", \"debut_alim\")\n",
    "    )\n",
    "    \n",
    "    # Convertir et enrichissement\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"debut_alim\", F.to_date(\"debut_alim\", \"yyyy-MM-dd\"))\n",
    "        .withColumn(\"updated_at\", F.current_timestamp())\n",
    "    )\n",
    "    \n",
    "    # Garder la ligne avec l'année la plus récente pour chaque cd_reseau\n",
    "    window_spec = Window.partitionBy([\"cd_reseau\",\"insee_commune\",\"quartier\"]).orderBy(F.col(\"annee\").desc())\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    "        .filter(F.col(\"row_number\") == 1)\n",
    "        .drop(\"row_number\")\n",
    "    )\n",
    "    # Liste finale des colonnes à garder\n",
    "    colonnes_a_garder = [\n",
    "        \"insee_commune\",\n",
    "        \"nom_commune\",\n",
    "        \"quartier\",\n",
    "        \"cd_reseau\",\n",
    "        \"nom_reseau\",\n",
    "        \"debut_alim\",\n",
    "        \"annee\",\n",
    "        \"updated_at\"\n",
    "    ]\n",
    "\n",
    "    # Sélectionner uniquement ces colonnes (drop tout le reste automatiquement)\n",
    "    df = df.select([c for c in colonnes_a_garder if c in df.columns])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2319e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_plv_silver(df):\n",
    "    df = (\n",
    "        df\n",
    "        .withColumnRenamed(\"cddept\", \"cd_dept\")\n",
    "        .withColumnRenamed(\"cdreseau\", \"cd_reseau\")\n",
    "        .withColumnRenamed(\"referenceprel\", \"reference_prel\")\n",
    "        .withColumnRenamed(\"dateprel\", \"date_prel\")\n",
    "        .withColumnRenamed(\"heureprel\", \"heure_prel\")\n",
    "        .withColumnRenamed(\"inseecommuneprinc\", \"insee_commune_princ\")\n",
    "        .withColumnRenamed(\"nomcommuneprinc\", \"nom_commune_princ\")\n",
    "        .withColumnRenamed(\"cdreseauamont\", \"cd_reseau_amont\")\n",
    "        .withColumnRenamed(\"nomreseauamont\", \"nom_reseau_amont\")\n",
    "        .withColumnRenamed(\"pourcentdebit\", \"pourcent_debit\")\n",
    "        .withColumnRenamed(\"plvconformitebacterio\", \"plv_conformite_bacterio\")\n",
    "        .withColumnRenamed(\"plvconformitechimique\", \"plv_conformite_chimique\")\n",
    "        .withColumnRenamed(\"plvconformitereferencebact\", \"plv_conformite_reference_bact\")\n",
    "        .withColumnRenamed(\"plvconformitereferencechim\", \"plv_conformite_reference_chim\")\n",
    "        .withColumnRenamed(\"conclusionprel\", \"conclusion_prel\")\n",
    "        .withColumnRenamed(\"ugelib\", \"unite_gestion\")\n",
    "        .withColumnRenamed(\"distrlib\", \"organisme_exploitant\")\n",
    "        .withColumnRenamed(\"moalib\", \"maitre_ouvrage\")\n",
    "    )\n",
    "    # Convertir et enrichissement\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"date_prel\", F.to_date(\"date_prel\", \"yyyy-MM-dd\"))\n",
    "        .withColumn(\"updated_at\", F.current_timestamp())\n",
    "    )\n",
    "    # Garder la ligne avec l'année la plus récente pour chaque cd_reseau\n",
    "    df = (\n",
    "        df\n",
    "        .dropDuplicates([\"cd_reseau\",\"insee_commune_princ\",\"date_prel\"])\n",
    "    )\n",
    "    # Liste finale des colonnes à garder\n",
    "    colonnes_a_garder = [\n",
    "        \"cd_dept\",\n",
    "        \"cd_reseau\",\n",
    "        \"reference_prel\",\n",
    "        \"date_prel\",\n",
    "        \"heure_prel\",\n",
    "        \"insee_commune_princ\",\n",
    "        \"nom_commune_princ\",\n",
    "        \"cd_reseau_amont\",\n",
    "        \"nom_reseau_amont\",\n",
    "        \"pourcent_debit\",\n",
    "        \"plv_conformite_bacterio\",\n",
    "        \"plv_conformite_chimique\",\n",
    "        \"plv_conformite_reference_bact\",\n",
    "        \"plv_conformite_reference_chim\",\n",
    "        \"conclusion_prel\",\n",
    "        \"unite_gestion\",\n",
    "        \"organisme_exploitant\",\n",
    "        \"maitre_ouvrage\",\n",
    "        \"updated_at\",\n",
    "        \"annee\"\n",
    "    ]\n",
    "\n",
    "    # Garder uniquement ces colonnes (drop tout le reste)\n",
    "    df = df.select([c for c in colonnes_a_garder if c in df.columns])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_result_silver(df):\n",
    "    df = (\n",
    "        df\n",
    "        .withColumnRenamed(\"cddept\", \"cd_dept\")\n",
    "        .withColumnRenamed(\"referenceprel\", \"reference_prel\")\n",
    "        .withColumnRenamed(\"cdparametresiseeaux\", \"cd_parametre_sise_eaux\")\n",
    "        .withColumnRenamed(\"cdparametre\", \"cd_parametre\")\n",
    "        .withColumnRenamed(\"libminparametre\", \"lib_parametre\")\n",
    "        .withColumnRenamed(\"libwebparametre\", \"lib_parametre_gp\")\n",
    "        .withColumnRenamed(\"qualitparam\", \"is_qualitatif\")\n",
    "        .withColumnRenamed(\"insituana\", \"insitu_analyse\")\n",
    "        .withColumnRenamed(\"rqana\", \"resultat_analyse\")\n",
    "        .withColumnRenamed(\"cdunitereferencesiseeaux\", \"cd_unite_reference_sise_eaux\")\n",
    "        .withColumnRenamed(\"cdunitereference\", \"cd_unite_reference\")\n",
    "        .withColumnRenamed(\"limitequal\", \"limite_qualite\")\n",
    "        .withColumnRenamed(\"refqual\", \"ref_qualite\")\n",
    "        .withColumnRenamed(\"valtraduite\", \"val_traduite\")\n",
    "        .withColumnRenamed(\"casparam\", \"cd_cas_param\")\n",
    "        .withColumnRenamed(\"referenceanl\", \"cd_ana_labo\")\n",
    "    )\n",
    "    # Convertir et enrichissement\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"updated_at\", F.current_timestamp())\n",
    "    )\n",
    "     # Transformer les O/N en True/False\n",
    "    df = df.withColumn(\n",
    "        \"is_qualitatif\",\n",
    "        F.when(F.upper(F.col(\"is_qualitatif\")) == \"O\", F.lit(True))\n",
    "         .when(F.upper(F.col(\"is_qualitatif\")) == \"N\", F.lit(False))\n",
    "         .otherwise(F.lit(None))\n",
    "    )\n",
    "    # Garder la ligne avec l'année la plus récente pour chaque cd_reseau\n",
    "    df = (\n",
    "        df\n",
    "        .dropDuplicates([\"cd_dept\",\"reference_prel\",\"cd_parametre\"])\n",
    "    )\n",
    "    # Liste des colonnes finales à garder\n",
    "    colonnes_a_garder = [\n",
    "        \"cd_dept\",\n",
    "        \"reference_prel\",\n",
    "        \"cd_parametre_sise_eaux\",\n",
    "        \"cd_parametre\",\n",
    "        \"lib_parametre\",\n",
    "        \"lib_parametre_gp\",\n",
    "        \"is_qualitatif\",\n",
    "        \"insitu_analyse\",\n",
    "        \"resultat_analyse\",\n",
    "        \"cd_unite_reference_sise_eaux\",\n",
    "        \"cd_unite_reference\",\n",
    "        \"limite_qualite\",\n",
    "        \"ref_qualite\",\n",
    "        \"val_traduite\",\n",
    "        \"cd_cas_param\",\n",
    "        \"cd_ana_labo\",\n",
    "        \"updated_at\",\n",
    "        \"annee\"\n",
    "    ]\n",
    "\n",
    "    # Garder uniquement ces colonnes (drop tout le reste automatiquement)\n",
    "    df = df.select([c for c in colonnes_a_garder if c in df.columns])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877708fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_delta_table(df, table_name: str, key_cols: list, partition_cols: list = [\"annee\"]):\n",
    "    \"\"\"\n",
    "    Simule un UPSERT (update + insert) sur une table Delta sans utiliser DeltaTable.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Nouveau DataFrame à fusionner.\n",
    "        table_name (str): Nom complet de la table cible (ex: 'silver.dis_plv').\n",
    "        key_cols (list): Liste des colonnes servant de clé unique.\n",
    "        partition_cols (list ): Colonne(s) de partition.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        schema_name = table_name.split(\".\")[0]\n",
    "        spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n",
    "\n",
    "        # Vérifie si la table existe déjà\n",
    "        table_exists = spark.catalog.tableExists(table_name)\n",
    "\n",
    "        if table_exists:\n",
    "            logger.info(f\"Fusion incrémentale dans '{table_name}'...\")\n",
    "\n",
    "            df_existing = spark.table(table_name)\n",
    "            # Union + suppression des doublons selon les clés métier\n",
    "            df_merged = (\n",
    "                df_existing.unionByName(df, allowMissingColumns=True)\n",
    "                .dropDuplicates(key_cols)\n",
    "            )\n",
    "        else:\n",
    "            logger.info(f\"Création de la table '{table_name}'...\")\n",
    "            df_merged = df\n",
    "\n",
    "\n",
    "        # Écriture finale (overwrite total car on a fusionné en mémoire)\n",
    "        (\n",
    "            df_merged.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .option(\"overwriteSchema\", \"true\")\n",
    "            .partitionBy(*partition_cols)\n",
    "            .saveAsTable(table_name)\n",
    "        )\n",
    "\n",
    "        logger.info(f\"Upsert Spark-only terminé pour '{table_name}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors de l'upsert dans '{table_name}': {e}\", exc_info=True)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096a98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_df_to_parquet(df, path: str, mode: str = \"overwrite\"):\n",
    "    \"\"\"\n",
    "    Écrit un DataFrame Spark en format Parquet dans le Data Lake.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Le DataFrame Spark à écrire.\n",
    "        path (str): Chemin complet dans le Data Lake (ex: '/mnt/datalake/bronze/dis_plv/').\n",
    "        mode (str): Mode d’écriture ('overwrite', 'append', etc.).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if df.isEmpty():\n",
    "            logger.warning(f\"Aucune donnée à écrire dans {path}.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Écriture en Parquet dans {path} (mode={mode})...\")\n",
    "        df.write.mode(mode).parquet(path)\n",
    "        logger.info(f\"Données stockées avec succès dans {path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur écriture Parquet dans {path}: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18e8cb0",
   "metadata": {},
   "source": [
    "# Récupération des données du schema Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ffa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com_bronze = spark.table(\"bronze.dis_com\")\n",
    "df_plv_bronze = spark.table(\"bronze.dis_plv\")\n",
    "df_result_bronze = spark.table(\"bronze.dis_result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb7b657",
   "metadata": {},
   "source": [
    "# Transformation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33539ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_com_silver = transform_com_silver(df_com_bronze)\n",
    "df_plv_silver = transform_plv_silver(df_plv_bronze)\n",
    "df_result_silver= transform_result_silver(df_result_bronze)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11003873",
   "metadata": {},
   "source": [
    "# Insertion dans le schema Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb264fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsert_delta_table(df_com_silver,\"silver.dis_com\",[\"cd_reseau\",\"insee_commune\",\"quartier\"],[\"annee\"])\n",
    "upsert_delta_table(df_plv_silver,\"silver.dis_plv\",[\"cd_reseau\",\"insee_commune_princ\",\"date_prel\"],[\"annee\",\"cd_dept\"])\n",
    "upsert_delta_table(df_result_silver,\"silver.dis_result\",[\"cd_dept\",\"reference_prel\",\"cd_parametre\"],[\"annee\",\"cd_dept\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7054f543",
   "metadata": {},
   "source": [
    "# Copie en parque dans le datalake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f30307",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_df_to_parquet(df_com_silver, f\"{MOUNT_POINT_SILVER}/dis_com/\")\n",
    "write_df_to_parquet(df_plv_silver, f\"{MOUNT_POINT_SILVER}/dis_plv/\")\n",
    "write_df_to_parquet(df_result_silver, f\"{MOUNT_POINT_SILVER}/dis_result/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
